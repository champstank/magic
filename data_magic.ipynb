{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import pickle\n",
    "import xgboost\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csc_matrix\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, accuracy_score, average_precision_score, f1_score, r2_score\n",
    "from sklearn.metrics import precision_score, recall_score, mean_absolute_error, mean_squared_error, median_absolute_error\n",
    "\n",
    "\n",
    "#====================================  SUPPORTING stateless functions  ==============================================#\n",
    "def is_regression(y):\n",
    "    \"\"\"\n",
    "    This function check to see if target is a regression\n",
    "    Params:\n",
    "        y np.array label array\n",
    "    Returns:\n",
    "        True\n",
    "    \"\"\"\n",
    "    try:\n",
    "        y_float = np.array(np.array(y,dtype=int),dtype=float)\n",
    "        return np.sum(np.abs(y_float - y))>0    # Is there an error with int conversion?\n",
    "    except:\n",
    "        return False                            # NOT regression, you through exception i.e. ['cat','dog']\n",
    "    \n",
    "def hashfile(filename):\n",
    "    \"\"\"\n",
    "    This function will create a hash for a file based on file content\n",
    "    Params:\n",
    "        filename string filename to be hashed\n",
    "    Returns:\n",
    "        string hashed file information\n",
    "    \"\"\"\n",
    "    BLOCKSIZE = 65536\n",
    "    hasher = hashlib.md5()\n",
    "    with open(filename, 'rb') as afile:\n",
    "        buf = afile.read(BLOCKSIZE)\n",
    "        while len(buf) > 0:\n",
    "            hasher.update(buf)\n",
    "            buf = afile.read(BLOCKSIZE)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def export_model(model,results,filename):\n",
    "    \"\"\"\n",
    "    This function will export model and results dictionary to disk for later use\n",
    "    Params:\n",
    "        model class model to be used\n",
    "        basename string model name\n",
    "        filename str filename being input\n",
    "    Returns:\n",
    "        True\n",
    "    \"\"\"\n",
    "    basename = filename.split('.')[0]\n",
    "    basename = basename.split('/')[-1]  # ignore folder names\n",
    "    # calculate the hash on filename\n",
    "    file_hash = hashfile(filename)\n",
    "    new_filename = basename + '_' + file_hash + '.p'\n",
    "    pickle.dump([model,results],open(new_filename,'wb'))\n",
    "    return\n",
    "      \n",
    "#==============================================  CLASS  ===========================================================#\n",
    "class data_magic():\n",
    "    \"\"\"This class will automate supervised regression & classification workflows\"\"\"\n",
    "    \n",
    "    def __init__(self,speed='fast',filename='None'):\n",
    "        nltk.download('wordnet')  #<< make sure this is downloaded\n",
    "        nltk.download('stopwords')\n",
    "        \n",
    "        self.filename = filename\n",
    "        self.df = None\n",
    "        self.speed = speed\n",
    "        self.data_type = None\n",
    "        self.analyzer = CountVectorizer().build_analyzer()\n",
    "        self.lang = \"english\"\n",
    "        self.stops = set(stopwords.words(self.lang))\n",
    "        self.stemmer = SnowballStemmer(self.lang)\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.X = None\n",
    "        self.y = None\n",
    "        self.macro_features = None\n",
    "        self.micro_features = None\n",
    "        self.results = {}\n",
    "        self.model = None\n",
    "        self.target_type = None\n",
    "        # run these below to make sure class is ready\n",
    "\n",
    "\n",
    "\n",
    "    def file_2_df(self):\n",
    "        \"\"\"\n",
    "        This function loads any file into a pandass dataframe. \n",
    "        Params:\n",
    "            filename string i.e. file.csv, file.xls\n",
    "        Returns:\n",
    "            df pandas.dataframe \n",
    "        \"\"\"\n",
    "        if self.filename.split('.')[-1].lower()=='csv':\n",
    "            print \"csv detected\"\n",
    "            self.df = pd.read_csv(self.filename)\n",
    "            self.data_type='csv'\n",
    "        elif self.filename.split('.')[-1].lower()=='tsv':\n",
    "            print 'text detected'\n",
    "            df = pd.read_csv(self.filename, sep='\\t')\n",
    "            self.df = df[df.columns[::-1]]\n",
    "            self.data_type='text'\n",
    "            print self.df[:2]   #data frame preview\n",
    "        elif (self.filename.split('.')[-1].lower()=='xls') or (self.filename.split('.')[-1].lower()=='xlsx'):\n",
    "            print \"excel detected\"\n",
    "            self.df = pd.read_excel(self.filename)\n",
    "            self.data_type='excel'\n",
    "        else:\n",
    "            print filename+\": is not supported yet!\"\n",
    "           \n",
    "        #Fix missing values, replace with np.nan\n",
    "        null_values = ['','?','-999']\n",
    "        for null_i in null_values:\n",
    "            self.df = self.df.replace(null_i,np.nan)\n",
    "        return self.df\n",
    "\n",
    "    def word_pipeline(self,word):\n",
    "        \"\"\"This function preprocesses the text to get ready for count vectorizer\n",
    "        Params:\n",
    "            word string word to be processed\n",
    "        Returns:\n",
    "            word string processed word\n",
    "        \"\"\"\n",
    "        #word = BeautifulSoup(word,\"html5lib\").get_text()       \n",
    "        word = re.sub(\"[^a-zA-Z]\", \" \", word) \n",
    "        word = word.lower() \n",
    "        word = self.stemmer.stem(word)\n",
    "        word = self.lemmatizer.lemmatize(word)\n",
    "        if word in self.stops:\n",
    "            word = None\n",
    "        return word\n",
    "\n",
    "    def process_words(self,doc):\n",
    "        \"\"\"This pipeline calls each word for count vectorizer\"\"\"\n",
    "        return (self.word_pipeline(w) for w in self.analyzer(doc))  \n",
    "    \n",
    "    def df_missing_value_injector(self,df):\n",
    "  \n",
    "        null_values = ['','?','-999']\n",
    "        \n",
    "        for null_i in null_values:\n",
    "            df = df.replace(null_i,np.nan)\n",
    "    \n",
    "        d_magic_string = 'NaN_789hghgg'\n",
    "        df.replace(null_values,d_magic_string, inplace=True)\n",
    "\n",
    "        micro_features = []\n",
    "        for i, col in enumerate(df.columns):\n",
    "            df_i = pd.get_dummies(df[col])\n",
    "            X_i = np.array(df_i.as_matrix(),dtype=float)\n",
    "\n",
    "            if d_magic_string in df_i.columns:\n",
    "                missing_data_index = list(df_i.columns).index(d_magic_string)\n",
    "                missing_data_filter = np.array(df_i[df_i.columns[missing_data_index]].values,dtype=int)==1\n",
    "                X_i[missing_data_filter,:] = float('NaN')\n",
    "                X_i[:,missing_data_index] = missing_data_filter\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            if i==0:\n",
    "                X = X_i\n",
    "            else:\n",
    "                X = np.hstack((X,X_i))\n",
    "                #print X.shape\n",
    "            micro_features.append(df_i.columns)\n",
    "           \n",
    "        return X, micro_features\n",
    "    \n",
    "    def df_classifier_input(self):\n",
    "        \"\"\"\n",
    "        This function will take a dataframe and split for binary\n",
    "        Params:\n",
    "            filename string i.e. file.csv, file.xls\n",
    "        Returns:\n",
    "            df pandas.dataframe \n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        macro_features = df.columns[0:-1]     # grab column names\n",
    "        target_label = df.columns[-1]         # grab target column name\n",
    "\n",
    "        # Process input features\n",
    "        if self.data_type=='text':  \n",
    "            from sklearn.pipeline import Pipeline\n",
    "            from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "            from scipy.sparse import hstack\n",
    "            from scipy.sparse import csr_matrix\n",
    "\n",
    "            print'This is a text, bag of words model'\n",
    "\n",
    "            def text_processor(df,analyzer='word'):\n",
    "                workflow = Pipeline([('vect', CountVectorizer(analyzer=analyzer)), ('tfidf',TfidfTransformer())])\n",
    "                text_list = list(df[df.columns[0]].values)\n",
    "                X = workflow.fit_transform(text_list)\n",
    "                if analyzer is 'word':\n",
    "                    macro_features = ['text']\n",
    "                else:\n",
    "                    macro_features = ['text_SL']   #SL = stemming & lemming\n",
    "                #/////////////////////////////////\n",
    "                cv = workflow.steps[0][1]\n",
    "                vocab_dict = cv.vocabulary_\n",
    "                id_vocab_dict={}\n",
    "                for key in vocab_dict:\n",
    "                    id_vocab_dict[str(vocab_dict[key])] = key \n",
    "                micro_features =[]\n",
    "                for word in range(0,len(id_vocab_dict)):\n",
    "                    micro_features.append(id_vocab_dict[str(word)])\n",
    "                return X,micro_features,macro_features\n",
    "\n",
    "            def process_sentiment(df):\n",
    "                analyzer = SentimentIntensityAnalyzer()\n",
    "                text_list = list(df[df.columns[0]].values)\n",
    "                data=[]\n",
    "                for msg in text_list:\n",
    "                    vs = analyzer.polarity_scores(msg)\n",
    "                    data.append([vs['neg'],vs['neu'],vs['pos'],vs['compound']])\n",
    "                X = csr_matrix(np.array(data))\n",
    "                micro_features = ['neg_sentiment','neu_sentiment','pos_sentiment','combined_sentiment']\n",
    "                macro_features = ['text_sentiment']\n",
    "                return X,micro_features,macro_features\n",
    "\n",
    "\n",
    "            X,micro_features,macro_features = text_processor(df)\n",
    "            \n",
    "            if self.speed=='slow':\n",
    "                X_2,micro_features_2,macro_features_2 = text_processor(df,analyzer=self.process_words)\n",
    "                X_3,micro_features_3,macro_features_3 = process_sentiment(df)   #calculate sentiment\n",
    "                #Combine all features\n",
    "                micro_features.append(micro_features_2)\n",
    "                micro_features.append(micro_features_3)\n",
    "                macro_features.append(macro_features_2)\n",
    "                macro_features.append(macro_features_3)\n",
    "                X = csc_matrix(hstack((X,X_2,X_3))) # hstacking sparse matrices changes type to COO, changing to CSR\n",
    "\n",
    "        else:\n",
    "            print 'NOT text'\n",
    "            X_df_dummy = pd.get_dummies(df[macro_features])     \n",
    "            micro_features = X_df_dummy.columns         # new columns names from tokenizer\n",
    "            \n",
    "            #X, micro_features = self.df_missing_value_injector(df[macro_features])\n",
    "            #print X.shape\n",
    "            X = X_df_dummy.as_matrix()                        # Numpy array, from dataframe\n",
    "\n",
    "        # Process Y\n",
    "        y_df = df[target_label]\n",
    "        y_raw = y_df.as_matrix()\n",
    "\n",
    "        if is_regression(y_raw):\n",
    "            print 'regression detected'\n",
    "            y = y_raw\n",
    "            self.target_type = 'reg'\n",
    "        else:\n",
    "            print 'classification detected'\n",
    "            y_df_dummy = pd.get_dummies(y_df)\n",
    "            y = np.argmax(y_df_dummy.as_matrix(),axis=1)\n",
    "            self.target_type = 'class'\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.macro_features = macro_features\n",
    "        self.micro_features = micro_features\n",
    "        return \n",
    "\n",
    "    def scoring(self,y,ypred_class,ypred_prob):\n",
    "        \"\"\"\n",
    "        This function will create a scored dictionary of your predictions\n",
    "        Params:\n",
    "            y np.array actual value\n",
    "            ypred_class np.array predicted values\n",
    "            ypred_prob np.array predicted probabilities of values\n",
    "        Returns:\n",
    "            results dict dictionary of all results \n",
    "        \"\"\"\n",
    " \n",
    "        #accuracy_score(y,ypred_class)\n",
    "        results={}\n",
    "        if self.target_type == 'reg':\n",
    "            print('Multiclass detected!')\n",
    "            results['neg_mean_absolute_error'] = mean_absolute_error(y,ypred_class)\n",
    "            results['neg_median_absolute_error'] =  median_absolute_error(y,ypred_class)\n",
    "            results['neg_squared_absolute_error'] = mean_squared_error(y,ypred_class)\n",
    "            results['r2'] = r2_score(y,ypred_class)\n",
    "            results['pearson-r'] = pearsonr(y,ypred_class)[0]\n",
    "            results['pearson-r-pval'] = pearsonr(y,ypred_class)[1]\n",
    "        else:   # must be binary\n",
    "            print('Binary detected!')\n",
    "            results['auc_score'] = roc_auc_score(y,ypred_prob)\n",
    "            results['accuracy'] = accuracy_score(y,ypred_class)\n",
    "            results['confusion_matrix'] = confusion_matrix(y,ypred_class)\n",
    "            results['pearson-r'] = pearsonr(y,ypred_prob)[0]\n",
    "            results['pearson-r-pval'] = pearsonr(y,ypred_prob)[1]\n",
    "            results['average_precision_score'] = average_precision_score(y,ypred_class)\n",
    "            results['f1_score'] = f1_score(y,ypred_class)\n",
    "            results['precision_score'] = precision_score(y,ypred_class)\n",
    "            results['recall_score'] = recall_score(y,ypred_class)\n",
    "            results['observation_count'] = len(y)\n",
    "            results['label_balance'] = np.mean(y)\n",
    "        self.results = results\n",
    "        return self.results\n",
    "    \n",
    "    def train_and_validate(self):    \n",
    "        \"\"\"\n",
    "        This function will train and cross validate your model\n",
    "        Params:\n",
    "            X np.array(matrix) input features\n",
    "            y np.array(vector) target \n",
    "            model class this is the model that will be trained and tested\n",
    "        Returns:\n",
    "            model class trained model object\n",
    "            scoring() function returns scored dictionary\n",
    "        \"\"\"\n",
    "\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        model = self.model\n",
    "\n",
    "        ypred_class = np.zeros_like(y,dtype=float)                     # initialize holder array, make sure it is float \n",
    "        ypred_prob = np.zeros_like(y,dtype=float)             \n",
    "        if is_regression(y):\n",
    "            from sklearn.model_selection import KFold\n",
    "            skf = KFold(n_splits=10, random_state=16)\n",
    "            for train_index, test_index in skf.split(X):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "                ypred_class[test_index] = model.predict(X_test)\n",
    "        else: # must be classification\n",
    "            skf = StratifiedKFold(n_splits=10, random_state=16)\n",
    "            for train_index, test_index in skf.split(X, y):\n",
    "                X_train, X_test = X[train_index], X[test_index]\n",
    "                y_train, y_test = y[train_index], y[test_index]\n",
    "                model.fit(X_train,y_train)\n",
    "\n",
    "                ypred_class[test_index] = model.predict(X_test)\n",
    "                ypred_prob[test_index] = model.predict_proba(X_test)[:,1]  \n",
    "        self.model = model\n",
    "        return self.model, self.scoring(y,ypred_class,ypred_prob)\n",
    "    \n",
    "    def model_search(self):\n",
    "        \"\"\"\n",
    "        This function will decide what type of model to use\n",
    "        Params:\n",
    "            model class trained model object\n",
    "            results dict trained model scoring information\n",
    "            filename str filename being input\n",
    "        Returns:\n",
    "            True\n",
    "        \"\"\"\n",
    "        X = self.X\n",
    "        y = self.y\n",
    "        \n",
    "        if self.target_type == 'reg':\n",
    "            if self.speed=='fast':\n",
    "                self.model = LinearRegression()\n",
    "            else: \n",
    "                self.model = xgboost.XGBRegressor()\n",
    "      \n",
    "        else:\n",
    "            if self.speed=='fast':\n",
    "                self.model = LogisticRegression()\n",
    "            else: \n",
    "                self.model = xgboost.XGBClassifier()\n",
    "            \n",
    "        if self.filename.split('.')[-1].lower()=='tsv':\n",
    "            pass\n",
    "        else:\n",
    "            imp = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "            pipe = Pipeline(steps=[('model', self.model)])\n",
    "\n",
    "            self.X[np.isnan(self.X)]=0\n",
    "\n",
    "            self.model = pipe\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "        This function will run entire workflow for prediction from file to model + results\n",
    "        Params:\n",
    "            filename str filename to be run\n",
    "        Returns:\n",
    "            True\n",
    "        \"\"\"\n",
    "\n",
    "        start_time = time.time()\n",
    "        self.file_2_df()                                    # load file into pandas dataframe\n",
    "        self.df_classifier_input()  # prepare data for ML\n",
    "        self.model_search()                         # model search data \n",
    "\n",
    "        train_model, results = self.train_and_validate()        # train model, cross validate and score\n",
    "        for key in results:\n",
    "            print \"     \",key,\":\",results[key]\n",
    "\n",
    "        #export_model(train_model,results, self.filename)                 # save model to disk\n",
    "        run_time = time.time() - start_time\n",
    "        print \"Ran in %.3f seconds\" % run_time\n",
    "        return True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
